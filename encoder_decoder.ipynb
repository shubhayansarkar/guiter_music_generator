{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da73926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('df_fored_samelength.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7af434f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [3, 6, 5, 3, 2, 6, 1, 4, 0, 1]\n",
       "1      [4, 2, 3, 2, 1, 2, 5, 0, 6, 4]\n",
       "2      [0, 5, 0, 5, 4, 2, 0, 3, 0, 5]\n",
       "3      [6, 6, 5, 0, 0, 5, 4, 6, 1, 0]\n",
       "4      [3, 4, 0, 2, 3, 4, 1, 0, 1, 0]\n",
       "                    ...              \n",
       "995    [6, 4, 6, 3, 5, 5, 1, 6, 3, 2]\n",
       "996    [6, 4, 3, 6, 6, 3, 5, 6, 5, 3]\n",
       "997    [6, 5, 2, 5, 6, 1, 5, 4, 1, 2]\n",
       "998    [4, 2, 1, 1, 4, 0, 0, 0, 3, 0]\n",
       "999    [2, 2, 0, 6, 4, 3, 3, 0, 2, 6]\n",
       "Name: first, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens=df['1st']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552504cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e65e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb624a8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_encoder_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, LSTM, Dense\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define an input sequence and process it.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m encoder_inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[43mnum_encoder_tokens\u001b[49m))\n\u001b[0;32m      6\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LSTM(latent_dim, return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m encoder_outputs, state_h, state_c \u001b[38;5;241m=\u001b[39m encoder(encoder_inputs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_encoder_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9f7837da",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps_in=10\n",
    "n_features=1\n",
    "train_size=850\n",
    "test_size=150\n",
    "LSTMoutputDimension=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51c23572",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'first'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\new1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\.conda\\envs\\new1\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\new1\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'first'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encoder_input_data\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m decoder_input_data\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\new1\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\.conda\\envs\\new1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'first'"
     ]
    }
   ],
   "source": [
    "encoder_input_data=df['first']\n",
    "decoder_input_data=df['second']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917776fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af72c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_predicted_data=decoder_input_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108af2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "randlist=[]\n",
    "for i in range(1000):\n",
    "    ranl=[]\n",
    "    for z in range(20):\n",
    "        ranl.append(random.randint(0,6))\n",
    "    randlist.append(ranl)\n",
    "df=pd.DataFrame({'data':randlist})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a685c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1,l2=[],[]\n",
    "for i in range(len(randlist)):\n",
    "    l1.append(randlist[i][:10])\n",
    "    l2.append(randlist[i][10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee1387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "encoder_input_data=np.array([l1])\n",
    "decoder_input_data=np.array([l2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb993a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=encoder_input_data.reshape(1000,10,1)\n",
    "decoder_input_data=decoder_input_data.reshape(1000,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bef0e75c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4b22451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec964494",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_predicted_data=decoder_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c682d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "# TRAINING WITH TEACHER FORCING\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs= Input(shape=(n_timesteps_in, n_features))\n",
    "encoder_lstm=LSTM(LSTMoutputDimension, return_state=True)\n",
    "LSTM_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "\n",
    "# We discard `LSTM_outputs` and only keep the other states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None, n_features), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(LSTMoutputDimension, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# Set up the decoder, using `context vector` as initial state.\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "#complete the decoder model by adding a Dense layer with Softmax activation function \n",
    "#for prediction of the next output\n",
    "#Dense layer will output one-hot encoded representation as we did for input\n",
    "#Therefore, we will use n_features number of neurons\n",
    "decoder_dense = Dense(n_features, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# put together\n",
    "model_encoder_training = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='model_encoder_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0f523a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_encoder_training\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 10, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  [(None, 40),         6720        ['input_9[0][0]']                \n",
      "                                 (None, 40),                                                      \n",
      "                                 (None, 40)]                                                      \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 40),   6720        ['decoder_inputs[0][0]',         \n",
      "                                 (None, 40),                      'lstm_7[0][1]',                 \n",
      "                                 (None, 40)]                      'lstm_7[0][2]']                 \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)          (None, None, 1)      41          ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,481\n",
      "Trainable params: 13,481\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_encoder_training.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_encoder_training.summary()\n",
    "# plot_model(model_encoder_training, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "55b78017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 29s 42ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 12ms/step - loss: 0.0000e+00 - accuracy: 0.1381 - val_loss: 0.0000e+00 - val_accuracy: 0.1440\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27bca54e4a0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model_encoder_training.fit([encoder_input_data, decoder_input_data], decoder_predicted_data,\n",
    "          batch_size=32,\n",
    "          epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0405a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b258131",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTMoutputDimension,))\n",
    "decoder_state_input_c = Input(shape=(LSTMoutputDimension,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6906df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Functions to generate Seq2Seq Dataset, one hot encode / decode Input & Output Sequences\n",
    "\n",
    "\n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# one hot encode sequence\n",
    "def one_hot_encode(sequence, n_unique):\n",
    "\tencoding = list()\n",
    "\tfor value in sequence:\n",
    "\t\tvector = [0 for _ in range(n_unique)]\n",
    "\t\tvector[value] = 1\n",
    "\t\tencoding.append(vector)\n",
    "\treturn array(encoding)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "  return [np.argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "  # prepare encoder data for the Encoder-Decoder training\n",
    "def get_encoder_triple(time_steps,vocabulary_size,verbose= False):\n",
    "  # generate random sequence\n",
    "  sequence_in = generate_sequence(time_steps, vocabulary_size)\n",
    "\n",
    "  encoder_in = sequence_in.copy()\n",
    "  \n",
    "\n",
    "   \n",
    "  \n",
    "\n",
    "  decoder_out = sequence_in[::-1]\n",
    "  \n",
    "  decoder_in = decoder_out.copy()\n",
    "  decoder_in.insert(0,0)\n",
    "  decoder_in.pop()\n",
    "\n",
    "  # one hot encode\n",
    "  X_encoder_in = one_hot_encode(encoder_in, vocabulary_size)\n",
    "  X_decoder_in = one_hot_encode(decoder_in, vocabulary_size)\n",
    "  y_decoder_out = one_hot_encode(decoder_out, vocabulary_size)\n",
    "  # reshape as 3D\n",
    "  X_encoder_in = X_encoder_in.reshape((1, X_encoder_in.shape[0], X_encoder_in.shape[1]))\n",
    "  X_decoder_in = X_decoder_in.reshape((1, X_decoder_in.shape[0], X_decoder_in.shape[1]))\n",
    "  y_decoder_out = y_decoder_out.reshape((1, y_decoder_out.shape[0], y_decoder_out.shape[1]))\n",
    "\n",
    "  if(verbose):\n",
    "    print('\\nSample X_encoder_in X_decoder_in and y_decoder_out')\n",
    "    print('\\nIn raw format:')\n",
    "    print('X_encoder_in=%s, X_decoder_in=%s, y_decoder_out=%s' % \n",
    "          (one_hot_decode(X_encoder_in[0]), one_hot_decode(X_decoder_in[0]), \n",
    "           one_hot_decode(y_decoder_out[0])))\n",
    "    print('\\nIn one_hot_encoded format:')\n",
    "    print('X_encoder_in=%s' % (X_encoder_in[0]))\n",
    "    print('X_decoder_in=%s' % (X_decoder_in[0]))\n",
    "    print('y_decoder_out=%s' % (y_decoder_out[0]))\n",
    "  return [array(X_encoder_in), array(X_decoder_in), array(y_decoder_out)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9f6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, n_features))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, 0] = 1\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_seq = list()\n",
    "    while not stop_condition:\n",
    "\n",
    "        # in a loop\n",
    "        # decode the input to a token/output prediction + required states for context vector\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # convert the token/output prediction to a token/output\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_digit = sampled_token_index\n",
    "        # add the predicted token/output to output sequence\n",
    "        decoded_seq.append(sampled_digit)\n",
    "        \n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_digit == '\\n' or\n",
    "           len(decoded_seq) == n_timesteps_in):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the input target sequence (of length 1) \n",
    "        # with the predicted token/output \n",
    "        target_seq = np.zeros((1, 1, n_features))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update input states (context vector) \n",
    "        # with the ouputed states\n",
    "        states_value = [h, c]\n",
    "\n",
    "        # loop back.....\n",
    "        \n",
    "    # when loop exists return the output sequence\n",
    "    return decoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f5f8268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input \t\t\t  Expected  \t   Predicted \t\tT/F\n",
      "1/1 [==============================] - 1s 794ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \t\t True\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "print('Input \\t\\t\\t  Expected  \\t   Predicted \\t\\tT/F')\n",
    "correct =0 \n",
    "sampleNo =  10\n",
    "for sample in range(0,sampleNo):\n",
    "  predicted= decode_sequence(encoder_input_data[sample].reshape(1,n_timesteps_in,n_features))\n",
    "  if (one_hot_decode(decoder_predicted_data[sample])== predicted):\n",
    "    correct+=1\n",
    "  print( one_hot_decode(encoder_input_data[sample]), '\\t\\t', \n",
    "        one_hot_decode(decoder_predicted_data[sample]),'\\t', predicted,\n",
    "        '\\t\\t',one_hot_decode(decoder_predicted_data[sample])== predicted)\n",
    "print('Accuracy: ', correct/sampleNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a218fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 809ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "seq=decode_sequence([[[1,5,4,7,9,4,5,6,7,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f2d21d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aeb94ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e917755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5fb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f914ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673dffb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d44a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a75dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430aea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93226718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a80dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Functions to generate Seq2Seq Dataset, one hot encode / decode Input & Output Sequences\n",
    "\n",
    "\n",
    "# generate a sequence of random integers\n",
    "def generate_sequence(length, n_unique):\n",
    "\treturn [random.randint(1, n_unique-1) for _ in range(length)]\n",
    "\n",
    "# one hot encode sequence\n",
    "def one_hot_encode(sequence, n_unique):\n",
    "\tencoding = list()\n",
    "\tfor value in sequence:\n",
    "\t\tvector = [0 for _ in range(n_unique)]\n",
    "\t\tvector[value] = 1\n",
    "\t\tencoding.append(vector)\n",
    "\treturn np.array(encoding)\n",
    "\n",
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "  return [np.argmax(vector) for vector in encoded_seq]\n",
    "\n",
    "  # prepare encoder data for the Encoder-Decoder training\n",
    "def get_encoder_triple(time_steps,vocabulary_size,verbose= False):\n",
    "  # generate random sequence\n",
    "  sequence_in = generate_sequence(time_steps, vocabulary_size)\n",
    "\n",
    "  encoder_in = sequence_in.copy()\n",
    "  \n",
    "\n",
    "   \n",
    "  \n",
    "\n",
    "  decoder_out = sequence_in[::-1]\n",
    "  \n",
    "  decoder_in = decoder_out.copy()\n",
    "  decoder_in.insert(0,0)\n",
    "  decoder_in.pop()\n",
    "\n",
    "  # one hot encode\n",
    "  X_encoder_in = one_hot_encode(encoder_in, vocabulary_size)\n",
    "  X_decoder_in = one_hot_encode(decoder_in, vocabulary_size)\n",
    "  y_decoder_out = one_hot_encode(decoder_out, vocabulary_size)\n",
    "  # reshape as 3D\n",
    "  X_encoder_in = X_encoder_in.reshape((1, X_encoder_in.shape[0], X_encoder_in.shape[1]))\n",
    "  X_decoder_in = X_decoder_in.reshape((1, X_decoder_in.shape[0], X_decoder_in.shape[1]))\n",
    "  y_decoder_out = y_decoder_out.reshape((1, y_decoder_out.shape[0], y_decoder_out.shape[1]))\n",
    "\n",
    "  if(verbose):\n",
    "    print('\\nSample X_encoder_in X_decoder_in and y_decoder_out')\n",
    "    print('\\nIn raw format:')\n",
    "    print('X_encoder_in=%s, X_decoder_in=%s, y_decoder_out=%s' % \n",
    "          (one_hot_decode(X_encoder_in[0]), one_hot_decode(X_decoder_in[0]), \n",
    "           one_hot_decode(y_decoder_out[0])))\n",
    "    print('\\nIn one_hot_encoded format:')\n",
    "    print('X_encoder_in=%s' % (X_encoder_in[0]))\n",
    "    print('X_decoder_in=%s' % (X_decoder_in[0]))\n",
    "    print('y_decoder_out=%s' % (y_decoder_out[0]))\n",
    "  return [np.array(X_encoder_in), np.array(X_decoder_in), np.array(y_decoder_out)]\n",
    "\n",
    "\n",
    "def create_encoder_dataset(train_size, test_size, time_steps,vocabulary_size, verbose= False):\n",
    "\n",
    "  X_encoder_in = list()\n",
    "  X_decoder_in = list()\n",
    "  y_decoder_out = list()\n",
    "\n",
    "  for _ in range(train_size):\n",
    "    triple=get_encoder_triple(time_steps,vocabulary_size) \n",
    "    X_encoder_in.append(triple[0])\n",
    "    X_decoder_in.append(triple[1])\n",
    "    y_decoder_out.append(triple[2])\n",
    "\n",
    "  X_encoder_in= np.array(X_encoder_in).squeeze()\n",
    "  X_decoder_in= np.array(X_decoder_in).squeeze()\n",
    "  y_decoder_out= np.array(y_decoder_out).squeeze()\n",
    "  if(verbose):\n",
    "    print('\\nGenerated sequence datasets as follows')\n",
    "    print('X_encoder_in.shape: ', X_encoder_in.shape)\n",
    "    print('X_decoder_in.shape: ', X_decoder_in.shape)\n",
    "    print('y_decoder_out.shape: ', y_decoder_out.shape)\n",
    "    print('Sample sequences in raw format:')\n",
    "    \n",
    "    print('X_encoder_in: \\n', one_hot_decode(X_encoder_in[0]))\n",
    "    print('X_decoder_in: \\n', one_hot_decode(X_decoder_in[0]))\n",
    "    print('y_decoder_out: \\n',one_hot_decode(y_decoder_out[0]))\n",
    "\n",
    "    print('Sample sequences in one-hot encoded format:')\n",
    "    print('X_encoder_in: \\n', X_encoder_in[0])\n",
    "    print('X_decoder_in: \\n', X_decoder_in[0])\n",
    "    print('y_decoder_out: \\n', y_decoder_out[0])\n",
    "\n",
    "  return X_encoder_in,X_decoder_in, y_decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5602328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sequence datasets as follows\n",
      "X_encoder_in.shape:  (5000, 10, 7)\n",
      "X_decoder_in.shape:  (5000, 10, 7)\n",
      "y_decoder_out.shape:  (5000, 10, 7)\n",
      "Sample sequences in raw format:\n",
      "X_encoder_in: \n",
      " [3, 5, 2, 4, 4, 3, 1, 2, 2, 3]\n",
      "X_decoder_in: \n",
      " [0, 3, 2, 2, 1, 3, 4, 4, 2, 5]\n",
      "y_decoder_out: \n",
      " [3, 2, 2, 1, 3, 4, 4, 2, 5, 3]\n",
      "Sample sequences in one-hot encoded format:\n",
      "X_encoder_in: \n",
      " [[0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "X_decoder_in: \n",
      " [[1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n",
      "y_decoder_out: \n",
      " [[0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_timesteps_in =   10#@param {type:\"integer\"}\n",
    "#each input sample has 4 values\n",
    "\n",
    "n_features = 7   #@param {type:\"integer\"}\n",
    "#each value is one_hot_encoded with 10 0/1\n",
    "#n_timesteps_out = 2  #@param {type:\"integer\"}\n",
    "#each output sample has 2 values padded with 0\n",
    "\n",
    "# generate random sequence\n",
    "#X,y = get_reversed_pairs(n_timesteps_in,  n_features, verbose=True)\n",
    "# generate datasets\n",
    "train_size= 5000 #@param {type:\"integer\"}\n",
    "test_size = 100  #@param {type:\"integer\"}\n",
    "\n",
    "LSTMoutputDimension=40\n",
    "\n",
    "encoder_input_data, decoder_input_data, decoder_predicted_data=\\\n",
    "create_encoder_dataset(train_size, test_size, n_timesteps_in,n_features , verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f90fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs= Input(shape=(n_timesteps_in, n_features))\n",
    "encoder_lstm=LSTM(LSTMoutputDimension, return_state=True)\n",
    "LSTM_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "\n",
    "# We discard `LSTM_outputs` and only keep the other states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None, n_features), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(LSTMoutputDimension, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# Set up the decoder, using `context vector` as initial state.\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "#complete the decoder model by adding a Dense layer with Softmax activation function \n",
    "#for prediction of the next output\n",
    "#Dense layer will output one-hot encoded representation as we did for input\n",
    "#Therefore, we will use n_features number of neurons\n",
    "decoder_dense = Dense(n_features, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# put together\n",
    "model_encoder_training = Model([encoder_inputs, decoder_inputs], decoder_outputs, name='model_encoder_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "801fd99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_encoder_training\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 10, 7)]      0           []                               \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None, 7)]    0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 40),         7680        ['input_2[0][0]']                \n",
      "                                 (None, 40),                                                      \n",
      "                                 (None, 40)]                                                      \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 40),   7680        ['decoder_inputs[0][0]',         \n",
      "                                 (None, 40),                      'lstm[0][1]',                   \n",
      "                                 (None, 40)]                      'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " decoder_dense (Dense)          (None, None, 7)      287         ['decoder_lstm[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,647\n",
      "Trainable params: 15,647\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_encoder_training.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_encoder_training.summary()\n",
    "# plot_model(model_encoder_training, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acac0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 13s 23ms/step - loss: 1.6982 - accuracy: 0.3000 - val_loss: 1.4629 - val_accuracy: 0.4027\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.2669 - accuracy: 0.5042 - val_loss: 1.0951 - val_accuracy: 0.5864\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.9328 - accuracy: 0.6647 - val_loss: 0.8137 - val_accuracy: 0.7178\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.7359 - accuracy: 0.7485 - val_loss: 0.6844 - val_accuracy: 0.7615\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.6174 - accuracy: 0.7962 - val_loss: 0.5903 - val_accuracy: 0.8048\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.5253 - accuracy: 0.8371 - val_loss: 0.5035 - val_accuracy: 0.8375\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.4603 - accuracy: 0.8620 - val_loss: 0.4157 - val_accuracy: 0.8829\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.4040 - accuracy: 0.8826 - val_loss: 0.3635 - val_accuracy: 0.9063\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.3521 - accuracy: 0.9035 - val_loss: 0.3360 - val_accuracy: 0.9063\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.2987 - accuracy: 0.9275 - val_loss: 0.2975 - val_accuracy: 0.9213\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.2741 - accuracy: 0.9317 - val_loss: 0.2680 - val_accuracy: 0.9323\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.2500 - accuracy: 0.9394 - val_loss: 0.2395 - val_accuracy: 0.9397\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.2185 - accuracy: 0.9516 - val_loss: 0.1950 - val_accuracy: 0.9605\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.1989 - accuracy: 0.9570 - val_loss: 0.1969 - val_accuracy: 0.9540\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.1763 - accuracy: 0.9658 - val_loss: 0.1772 - val_accuracy: 0.9614\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.1755 - accuracy: 0.9606 - val_loss: 0.1687 - val_accuracy: 0.9604\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1516 - accuracy: 0.9699 - val_loss: 0.1428 - val_accuracy: 0.9703\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1488 - accuracy: 0.9665 - val_loss: 0.1883 - val_accuracy: 0.9434\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.1363 - accuracy: 0.9719 - val_loss: 0.1325 - val_accuracy: 0.9732\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.1335 - accuracy: 0.9705 - val_loss: 0.1104 - val_accuracy: 0.9809\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.1312 - accuracy: 0.9698 - val_loss: 0.1196 - val_accuracy: 0.9738\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.1032 - accuracy: 0.9821 - val_loss: 0.1152 - val_accuracy: 0.9753\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0986 - accuracy: 0.9826 - val_loss: 0.0952 - val_accuracy: 0.9823\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0957 - accuracy: 0.9820 - val_loss: 0.0889 - val_accuracy: 0.9861\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.0946 - accuracy: 0.9814 - val_loss: 0.0772 - val_accuracy: 0.9888\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.1012 - accuracy: 0.9766 - val_loss: 0.1155 - val_accuracy: 0.9708\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0867 - accuracy: 0.9828 - val_loss: 0.0883 - val_accuracy: 0.9800\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0805 - accuracy: 0.9849 - val_loss: 0.0711 - val_accuracy: 0.9888\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0755 - accuracy: 0.9863 - val_loss: 0.0777 - val_accuracy: 0.9836\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 0.0798 - accuracy: 0.9835 - val_loss: 0.0869 - val_accuracy: 0.9792\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0692 - accuracy: 0.9872 - val_loss: 0.0763 - val_accuracy: 0.9841\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.0794 - accuracy: 0.9813 - val_loss: 0.0573 - val_accuracy: 0.9917\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.0614 - accuracy: 0.9893 - val_loss: 0.0596 - val_accuracy: 0.9895\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.0665 - accuracy: 0.9866 - val_loss: 0.0792 - val_accuracy: 0.9790\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0604 - accuracy: 0.9888 - val_loss: 0.0841 - val_accuracy: 0.9789\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0611 - accuracy: 0.9881 - val_loss: 0.0480 - val_accuracy: 0.9927\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0567 - accuracy: 0.9892 - val_loss: 0.0980 - val_accuracy: 0.9722\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0703 - accuracy: 0.9835 - val_loss: 0.0476 - val_accuracy: 0.9929\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0479 - accuracy: 0.9920 - val_loss: 0.0674 - val_accuracy: 0.9841\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0536 - accuracy: 0.9890 - val_loss: 0.0543 - val_accuracy: 0.9888\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.0440 - accuracy: 0.9923 - val_loss: 0.0618 - val_accuracy: 0.9855\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0580 - accuracy: 0.9864 - val_loss: 0.0641 - val_accuracy: 0.9827\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0400 - accuracy: 0.9940 - val_loss: 0.0427 - val_accuracy: 0.9917\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0463 - accuracy: 0.9908 - val_loss: 0.0688 - val_accuracy: 0.9814\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 0.0446 - accuracy: 0.9913 - val_loss: 0.0347 - val_accuracy: 0.9949\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0590 - accuracy: 0.9848 - val_loss: 0.1151 - val_accuracy: 0.9642\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0469 - accuracy: 0.9906 - val_loss: 0.0344 - val_accuracy: 0.9949\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 0.0291 - accuracy: 0.9966 - val_loss: 0.0334 - val_accuracy: 0.9948\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 1s 12ms/step - loss: 0.0662 - accuracy: 0.9812 - val_loss: 0.0461 - val_accuracy: 0.9905\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 0.0342 - accuracy: 0.9941 - val_loss: 0.0419 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263340dded0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/encode_decode.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "model_encoder_training.fit([encoder_input_data, decoder_input_data], decoder_predicted_data,\n",
    "          batch_size=32,\n",
    "          epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddc665ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10766f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(LSTMoutputDimension,))\n",
    "decoder_state_input_c = Input(shape=(LSTMoutputDimension,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93bbd148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input \t\t\t  Expected  \t   Predicted \t\tT/F\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 563ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "[3, 5, 2, 4, 4, 3, 1, 2, 2, 3] \t\t [3, 2, 2, 1, 3, 4, 4, 2, 5, 3] \t [3, 2, 2, 1, 3, 4, 4, 2, 5, 3] \t\t True\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[5, 2, 4, 6, 5, 3, 3, 1, 4, 5] \t\t [5, 4, 1, 3, 3, 5, 6, 4, 2, 5] \t [5, 4, 1, 3, 3, 5, 6, 4, 2, 5] \t\t True\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "[5, 1, 6, 2, 6, 3, 1, 2, 4, 5] \t\t [5, 4, 2, 1, 3, 6, 2, 6, 1, 5] \t [5, 4, 2, 1, 3, 6, 2, 6, 1, 5] \t\t True\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "[3, 2, 1, 3, 3, 4, 1, 5, 1, 1] \t\t [1, 1, 5, 1, 4, 3, 3, 1, 2, 3] \t [1, 1, 5, 1, 4, 3, 3, 1, 2, 3] \t\t True\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[3, 4, 4, 2, 3, 3, 2, 6, 6, 5] \t\t [5, 6, 6, 2, 3, 3, 2, 4, 4, 3] \t [5, 6, 6, 2, 3, 3, 2, 4, 4, 3] \t\t True\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[4, 5, 1, 2, 1, 3, 1, 6, 3, 6] \t\t [6, 3, 6, 1, 3, 1, 2, 1, 5, 4] \t [6, 3, 6, 1, 3, 1, 2, 1, 5, 4] \t\t True\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "[1, 4, 6, 5, 2, 3, 4, 4, 2, 3] \t\t [3, 2, 4, 4, 3, 2, 5, 6, 4, 1] \t [3, 2, 4, 4, 3, 2, 5, 6, 4, 1] \t\t True\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[6, 1, 5, 1, 4, 5, 4, 4, 6, 1] \t\t [1, 6, 4, 4, 5, 4, 1, 5, 1, 6] \t [1, 6, 4, 4, 5, 4, 1, 5, 1, 6] \t\t True\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "[5, 4, 4, 4, 4, 6, 1, 1, 5, 2] \t\t [2, 5, 1, 1, 6, 4, 4, 4, 4, 5] \t [2, 5, 1, 1, 6, 4, 4, 4, 4, 5] \t\t True\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "[4, 2, 5, 1, 4, 5, 2, 2, 2, 1] \t\t [1, 2, 2, 2, 5, 4, 1, 5, 2, 4] \t [1, 2, 2, 2, 5, 4, 1, 5, 2, 4] \t\t True\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "print('Input \\t\\t\\t  Expected  \\t   Predicted \\t\\tT/F')\n",
    "correct =0 \n",
    "sampleNo =  10\n",
    "for sample in range(0,sampleNo):\n",
    "  predicted= decode_sequence(encoder_input_data[sample].reshape(1,n_timesteps_in,n_features))\n",
    "  if (one_hot_decode(decoder_predicted_data[sample])== predicted):\n",
    "    correct+=1\n",
    "  print( one_hot_decode(encoder_input_data[sample]), '\\t\\t', \n",
    "        one_hot_decode(decoder_predicted_data[sample]),'\\t', predicted,\n",
    "        '\\t\\t',one_hot_decode(decoder_predicted_data[sample])== predicted)\n",
    "print('Accuracy: ', correct/sampleNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae208cc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_4\" is incompatible with the layer: expected shape=(None, 10, 7), found shape=(None, 1, 10)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m seq\u001b[38;5;241m=\u001b[39m\u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_sequence\u001b[39m(input_seq):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Encode the input as state vectors.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     states_value \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Generate empty target sequence of length 1.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_features))\n",
      "File \u001b[1;32m~\\.conda\\envs\\new1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\SHUBHA~1\\AppData\\Local\\Temp\\__autograph_generated_fileb6btj1nr.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_4\" is incompatible with the layer: expected shape=(None, 10, 7), found shape=(None, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "seq=decode_sequence([[[1,5,4,7,9,4,5,6,7,3]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5da045e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 5, 1, 2, 3, 5, 6, 2, 1]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d7e04f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save('C:\\mine\\guitar_notes_classification\\saved_models\\encoder_Decoder.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5201159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "encoder_model=load_model('C:\\mine\\guitar_notes_classification\\saved_models\\encoder_decoder.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b1864b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_4\" is incompatible with the layer: expected shape=(None, 10, 7), found shape=(None, 1, 10)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m34\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m56\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m, in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_sequence\u001b[39m(input_seq):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Encode the input as state vectors.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     states_value \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Generate empty target sequence of length 1.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, n_features))\n",
      "File \u001b[1;32m~\\.conda\\envs\\new1\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\SHUBHA~1\\AppData\\Local\\Temp\\__autograph_generated_fileb6btj1nr.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Shubhayan\\.conda\\envs\\new1\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_4\" is incompatible with the layer: expected shape=(None, 10, 7), found shape=(None, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "decode_sequence([[[1,34,56,1,1,3,4,5,6,7]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b283196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
